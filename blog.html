<!DOCTYPE html>
<html>
<head>
  <title>Blog</title>
</head>
<body>
  <h1>My first principle of prompt engineering</h1>
  <p>I’ve noticed a pattern with people who either warn me about using ChatGPT or mess up catastrophically when they try to use it themselves. It’s not that the tool is bad; it’s that they’re treating it like some kind of magical oracle. They type in a vague question, get a vague answer, and then decide either (a) ChatGPT is the worst thing ever or (b) this slightly confused response is now their gospel truth. Neither of these reactions is ideal.

When I use ChatGPT, I treat it like I’m working with a well-meaning but somewhat forgetful assistant. If I need it to write an essay problem about subject matter jurisdiction in federal civil procedure, I don’t just ask for the essay problem. I start by testing it: “What do you know about subject matter jurisdiction?” Then I compare its answer to my notes, which—shocking plot twist—I actually check because I’m aware that ChatGPT doesn’t have a law degree. This lets me see if it knows what I’m talking about before I trust it to generate anything useful.

Think of it like using a search engine. If you want to find the “Parable of the Law” from Kafka’s The Trial, you could just type “parable of the law” into Google and hope for the best. Sometimes that works. Sometimes it doesn’t. A better method might be to search for “Kafka The Trial,” find the text, and then hit Ctrl+F to hunt for the specific passage. If you only try the first method and fail, you might conclude that Google is useless, but that’s not really fair. The tool works—you just didn’t use it in a way that plays to its strengths.

This applies to ChatGPT too. When I say I use it, people seem to assume I’m just typing questions into the void and trusting whatever comes back. But I don’t even trust encyclopedias or dictionaries without double-checking. If I look something up in a reference book, I often go search for a second opinion because, let’s face it, even trusted sources sometimes get things wrong. That’s not distrust; it’s just basic intellectual hygiene.

The way I see it, the most basic form of “prompt engineering” is really just asking good questions, thinking critically about the answers, and following up with more questions if something seems off. It’s the same skill you’d use in any research setting. Tools like ChatGPT or search engines aren’t here to hand you perfect answers—they’re here to help you find your way to better ones. If you’re not willing to put in the effort to guide them, you’ll probably end up frustrated or misled.

The problem isn’t the tools; it’s how people approach them. If you expect ChatGPT to churn out fully polished, perfectly accurate work without any oversight, you’re going to have a bad time. But if you treat it like a collaborator—one that needs a little coaching—it can be a powerful addition to your workflow. In fact, I’d argue that the process of refining and testing its output forces you to think more clearly and critically, which is a good habit no matter what you’re working on.

So no, I don’t see ChatGPT as a magic oracle. I see it as an assistant who’s always willing to help, a little prone to confusion, and occasionally brilliant when you give it the right instructions. Maybe it’s not perfect, but neither am I. We get along just fine.</p>
</body>
</html>
