<!DOCTYPE html>
<html lang="en">    
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Nicholas Stetler</title>
    <style>
body {
    font-family: Georgia, serif;
    line-height: 1.7;
    max-width: 800px;
    margin: 40px auto;
    padding: 20px;
}

h1, h2 {
    font-family: Georgia, serif;
    font-weight: bold;
}

h1 {
    font-size: 28px;
}

h2 {
    font-size: 24px;
    border-bottom: 2px solid #ccc;
    padding-bottom: 5px;
}

p {
    font-size: 18px;
}
    </style>
</head>
    <section id="about">
        <h1>About me</h1>
        <p>I am a law student interested in computers, old books, and public policy.</p>
    </section>
     <section id="blog">
         <h1>Blog</h1>
<body>
    <h3>Policy Brief:Reinsurance as Scalable Governance for Frontier AI Risk </h3>
    <p><strong>Author:</strong> Nicholas A. Stetler, JD Candidate 2026, SIU Simmons Law</p>

    <h5>Overview</h5>
    <p>Frontier AI systems—especially large models with emergent capabilities or dual-use potential—pose catastrophic downside risks that existing regulatory frameworks are ill-equipped to manage. This brief proposes a targeted federal reinsurance program as a mechanism to support the growth of a private insurance market for high-risk AI, thereby transforming currently unmanageable uncertainty into structured, incentivized governance.</p>

    <h5>The Proposal</h5>
    <p>Congress should authorize a federally funded reinsurance scheme for insurers who underwrite AI developers working with the most dangerous AI systems. Modeled in part on the Price-Anderson Act (for nuclear energy) and the FDIC (for banking), this program would cap insurers' catastrophic exposure while preserving private sector judgment about underwriting and pricing.</p>
    <p>By absorbing tail risk, the federal government would allow insurers to offer meaningful coverage, and in doing so, exert pressure on developers to adopt safety practices, restrict certain behaviors, and disclose risk-related information. Insurers, in turn, would generate granular risk mappings through actuarial processes, exclusions, and coverage thresholds—information that regulators and the public could use to better understand the landscape of emerging AI hazards.</p>

    <h5>Why This Matters</h5>
    <ul>
        <li><strong>Targeted Governance:</strong> The program would apply only to systems that meet clear criteria for extreme capability or scale (e.g., SB 1047’s "frontier model" definition), avoiding overreach while addressing the most urgent class of risks.</li>
        <li><strong>Scalable & Incentive-Compatible:</strong> Unlike rigid command-and-control regimes, reinsurance works with market structures, producing a governance layer that adapts over time.</li>
        <li><strong>Knowledge Generation:</strong> The insurance underwriting process transforms unknown unknowns into quantifiable liabilities, aiding the broader project of AI alignment and public safety.</li>
    </ul>

    <h5>Strategic Rationale</h5>
    <p>Well-designed insurance markets can do more than compensate for loss—they can shape behavior upstream by influencing which practices are insurable and which are excluded. Federally backed reinsurance catalyzes this market where private actors would otherwise be unwilling to bear systemic risk alone. By focusing narrowly on frontier systems, the program ensures clarity, minimizes administrative burden, and builds an initial testbed for broader AI governance mechanisms.</p>

    <h5>Conclusion</h5>
    <p>Just as the insurance revolution enabled empire-building exploration and industrial risk-taking in earlier eras, reinsurance for high-risk AI could serve as a 21st-century governance foundation. By linking safety incentives to insurability and enabling risk assessment at scale, a targeted federal reinsurance program offers a pragmatic, adaptive, and forward-compatible approach to regulating the most dangerous AI systems.</p>

    <h3>Conclusion I wanted to write for my reinsurance paper</h3> 
    <p>I had so much fun writing this for my conclusion that I can't bear to throw it out. Unfortunately, I can't use it because it is nowhere near the appropriate tone.</p>
    <p>Perhaps we stand today where the world once stood before the steam engine turned its first wheel, before the atom split, before the first bulb flickered on in the darkness.</p>
    
    <p>There was a time, not so long ago, when another frontier beckoned. A time when vast fortunes were wagered on what lay beyond the horizon. The Age of Exploration was not born from curiosity alone—it was born from risk, and from the need to manage it. When the great powers of Europe set their ships toward unknown waters, they did so not blindly, but backed by something new: Insurance.</p>
    
    <p>Lloyd’s of London, rising from a coffeehouse filled with men who understood that the future belonged not to those who feared the storm, but to those who prepared for it. Policies written on parchment, staking gold on the gamble that a ship might return not shattered on the rocks, but laden with spice and silk, gold and silver. Insurance turned exploration from recklessness into strategy, from a fatal wager into an empire-building enterprise. It was not the compass or the sextant alone that carried men across the oceans—it was the knowledge that, should the worst happen, the risk had already been accounted for.</p>
    
    <p>And now, here we stand, staring at a new, uncharted ocean. The ghost in the machine is no longer a thought experiment. It is no longer theoretical. It is being built, here and now, by minds and machines operating at a speed the world has never seen. And just like the great voyages of the past, it carries risks that could shape, or shake, the foundations of civilization itself.</p>
    
    <p>History does not wait. It does not hesitate. It rewards those who prepare, and it punishes those who do not. And if we are to cross this next great ocean, there is no better preparation—nothing more proactive, nothing more prudent—than insurance.</p>
    
    <h3>What Robert Caro can teach us about AI governance</h3>
    
    <p>Over the past year I finally finished <em>The Power Broker</em> after rediscovering the joy of audiobooks. I listened to <em>Working</em> then moved through <em>The Years of Lyndon Johnson</em> (four volumes) in an odd order. I started with <em>Master of the Senate</em> (v.3), then moved to <em>The Path to Power</em> (v.1), then <em>The Passage of Power</em> (v.4), and finally <em>Means of Ascent</em> (v.2). I then went back over <em>Working</em>. I have now been working my way through Peter Watson’s two books on intellectual history, <em>Ideas: From Fire to Freud</em>, and <em>The Modern Mind</em>. I also just wrote a potted history of the Civil Rights movement and then a comparison of Kafka’s sense of alienation and the alienation of Black folks after the passage of the 1964 Civil Rights Act. Currently, I am working on my law journal note on reinsurance as a mechanism of AI governance, as getting AI right is really the great challenge of our time. I see a through-line between each of these topics that I will try to elaborate here.</p>
    
    <p>Robert Caro doesn’t just write biographies—he writes about power. His books aren’t just about Robert Moses or Lyndon Johnson; they’re about how things actually get done in America. If you want to change the world through policy, you need more than just good ideas. You need to understand how power moves through institutions and how to make it work for you. His work shows that successful policymaking depends on three key elements: first, substantively good policy—it has to work in the real world, not just look good on paper. Second, a compelling narrative—people need to believe in it, or it won’t go anywhere. And third, relentless realpolitik—you have to navigate bureaucracies, alliances, and opposition to make it happen.</p>
    
    <p>This framework isn’t just useful for looking at the past—it’s essential for thinking about the future, including how we govern artificial intelligence. AI is moving fast, and if we want to regulate it effectively, we need to apply the same lessons that Caro’s subjects mastered.</p>
    
    <p>First, a policy has to actually work. It has to be enforceable, clear, and designed to achieve real results. This sounds obvious, but you’d be surprised how many policies fail because they weren’t thought through at a technical level. Caro’s subjects understood this well. Robert Moses wrote policies so airtight that even his political enemies couldn’t undo them. He knew the legal and financial levers of power better than anyone. Lyndon Johnson, meanwhile, left the technical details to experts, but he made sure the policies he pushed had real-world impact. AI policy has to follow this approach. It needs to be technically precise—a vague liability law won’t stop AI risks. It has to be legally enforceable—rules that can’t be applied in practice are useless. And it must be aligned with incentives—if a policy goes against the interests of key players, they’ll find ways around it.</p>
    
    <p>But good policy isn’t enough. You also need a story that makes people care, a sticky why. Every major political movement has framed its policies within a broader vision. Ancient Rome had <em>Roma Eterna</em>—The Eternal Rome. The Soviet Union had <em>Seize the Means of Production!</em> The United States has the American Dream and The Great Society. Thinkers in the liberal/progressive tradition such as Locke, Rawls, and Arendt all understood that political legitimacy depends on people believing in the system. A policy without a strong narrative is just paperwork. AI regulation is no different. If it’s framed as blocking innovation, it will fail. If it’s framed as ensuring human control over powerful technology, it has a chance. The way policymakers talk about AI will shape how people react to regulation.</p>
    
    <p>Even if a policy is well-designed and has a great narrative, it still won’t pass on its own. Someone has to push it through, navigate resistance, and make it stick. Robert Moses was a master of bureaucracy—he used legal loopholes and bureaucratic authority to push through his projects. Lyndon Johnson understood people—he knew how to build coalitions, cut deals, and use pressure to get things done. AI policymakers need to apply these same lessons. They need to understand what politicians, businesses, and voters actually want. AI regulation won’t pass without buy-in from key players—tech companies, advocacy groups, and governments. Even the best AI policy will fail if it doesn’t fit into the real-world incentives of lawmakers.</p>
    
    <p>Big policy changes often disrupt existing narratives, which can lead to backlash. Caro shows how leaders who changed the system had to manage these disruptions carefully. Kafka’s world feels like a bureaucratic nightmare—laws change, but people still feel powerless. This isn’t so different from the post-Civil Rights era, where the legal landscape changed, but people’s lived experiences didn’t always follow immediately. AI is going to change society in unpredictable ways, and that can create backlash. If AI-generated content dominates books, movies, and art, will people still see it as real? Policymakers need to be ahead of the curve in managing public reaction to AI.</p>
    
    <p>The lessons from Caro’s books apply directly to AI governance. AI regulations need to be clear and enforceable. AI governance must be framed in a way that resonates with the public. Policymakers need to understand power dynamics to get AI laws passed. Robert Moses knew that the technical side of policy is critical, but so is shaping the bureaucracy to implement it. Lyndon Johnson understood that a strong vision is important, but execution matters just as much. And Kafka’s world reminds us that big shifts in technology and policy create societal disorientation—ignoring this leads to backlash.</p>
    
    <p>Policymakers need to recognize that good ideas alone aren’t enough. To succeed, AI policy must be:</p>
    <ul>
        <li><strong>Technically effective</strong>—it must work in practice, not just in theory.</li>
        <li><strong>Politically persuasive</strong>—it must be framed in a way that makes sense to people.</li>
        <li><strong>Strategically implemented</strong>—it must navigate the realities of government, business, and society.</li>
    </ul>
    
    <p>If AI policy is going to succeed, it needs the same mix of technical know-how, compelling vision, and strategic execution that Caro’s subjects used to reshape the world. The future of AI governance depends on policymakers who understand not just the technology, but the politics of making real change happen.</p>
</body>
    <section id="Contact">
        <h1>Contact</h1>
        <p>nickstetler@gmail</p>
    </section>
    <section>
        <h1>Curriculum Vitæ</h1>
         <a href="CV_NICHOLAS_STETLER.pdf" target="_blank">Curriculum Vitæ</a>
    </section>
</html>
